Statistical language modelling is a technique that attempts to measure how likely any given piece of text is by building a probabilistic model and assigning a probability value to the text. These models are generally very large and are generated from a very large collection of documents.

A statistical language model is a probability distribution $P(s)$ over possible sentences $S$.\footnote{In this thesis sentences represent linguistic units, such as spoken utterances, or in some case even documents.} Its task can be shown with an example from automated speech recognition (ASR)\footnote{ASR is the process of converting a speech signal to a sequence of words, by means of an algorithm implemented as a computer program.}. Imagine that one day you walk on the beach, and you find a bottle. Although that in itself is not really surprising, you also find that the bottle contains a note. You can make out the script; the language is unknown to you, but in the address you recognise the sender's country. Eager as you are, you want to decipher the message. With sophisticated translation services as Google Translate you could at least get an idea of what the note says in an instant. These translation services work bidirectionally, but you decide to learn the language, as to write your reply.

One of the first things you will probably do is getting started with some basic words to fill you vocabulary. We call the vocabulary $V$, and the number of words you've learned already $W$. By learning to count, you first have to learn the translation for \emph{one}. Then you can go on to learn \emph{two}. By the time you get to \emph{ten}, you have counted up to \emph{nine} a lot of times already. Counting is then not only a matter of coming up with the right translation, but it is also about memory. Coming up with \emph{ten} is probably easier if you have just counted to \emph{nine}.

For statistical language models this is the same. Producing the translation of a word $w$, goes right with the probability $p(w)=\frac{1}{W}$, as there is no prior knowledge. If we tell the system that it has to produce the translation of a number between \emph{one} and \emph{ten}, then chance that he has it right if it memorised all ten digits is $p(w)=\frac{1}{10}$. If $W$ is much larger than $10$, this is really an improvement. If we now ask the system to finish the following sequence: \emph{one} \emph{two} \emph{three} $\ldots$, it can choose between two strategies. The first one is just filling in one of the $W$ words. A smarter approach however, is to use the information of the sequence. Let's call this sequence $s = (w_1, w_2, w_3)$ and we want to predict $w_4$. We want the word $w_4$ that out of all words in $V$ has the highest likelihood of following $s$: $w_4 = \argmax_{w}^{V} p(w|s)$. So with a statistical language model we can investigate how likely a word is given its context, and if we do that for each word we know, we can also pick the best one.

But this method has also some downsides. First, since most languages have an unbounded number of words, it is impossible to know the complete vocabulary. This has the consequence that not only you do not recognise the word, you also cannot evaluate its likelihood. The second problem is that since we model probabilities of words, we are never sure. This is especially true if we also want to model for out-of-vocabulary (OOV) words\footnote{Out-of-vocabulary words are unknown words that can either be learned (added to the vocabulary) or be denoted as a special word that will never occur in the vocabulary.}, for which we then must sacrifice a part of the probability space. For a human it is hard to say exactly how likely $p(w_4|s)$ is, but for a computer this is just the result of a computation. Another downside of having an unbounded number of words is that the vocabulary can get very big. As we saw, we do not want to only store the words, but also other information that allow us the make an educated guess. This can range from having knowledge of the previous words, but also syntax rules, or knowing the specific domain for which you are computing the word probabilities help. All these context information takes a lot of memory, both for the translator as for the computer.

For the rest of this thesis, we assume that we learn the language by reading a lot. This ignores other valuable resources such as grammar, speech, and cultural influences. But we have a dictionary, so we can at least get an idea of what's in the text. Again, we focus on two things. First we want to recognise the words, and second, we want to recognise patterns that preceed a word. This will be our approximation of the language. This approximation of patterns and their words can be modelled in many ways. In the next section we introduce the models that are the foundation of this thesis.

%\begin{itemize}
%	\item What is SLM?
%    \item Why do we do it?
%    \item What else is there?
%\end{itemize}

\section{Patterns in Language}
The goal of a statistical language model to assign a probability to a sequence of words. Say we have a sentence $s$ of $m=5$ words: \emph{Bananas are his favourite fruit.} If we want to compute the query likelihood of $s$, $p(s)$, we have to start 
at the first word: \emph{Bananas}. For now assume that we know every word that we might encounter. What is the chance that the first word was \emph{Bananas}? $p(\mathit{Bananas})$. The same goes for the second word: the chance of the second word being \emph{are} is $p(\mathit{are})$. We can do this for every word $m_i$ in $s$. This results in\footnote{$p(s)=\prod_{i=1}^m P(w_i)$}
\begin{equation}
P(s) = P(\mathit{Bananas})P(\mathit{are})P(\mathit{his})P(\mathit{favourite})P(\mathit{fruit})\label{eq:unigramexample}
\end{equation}

In \cref{eq:unigramexample} we use our knowledge of how likely it is to use each of the words. But as clear from the example, it does not take into relation the word order\footnote{The sentence \emph{his are favourite Bananas fruit} yields the same query likelihood, but lacks semantic and syntactic coherency.}, nor the context in which the words where said.

This very simple and naive language model is called a unigram language model\footnote{Henceforth is we refer to \emph{grams} we mean words, unless stated otherwise.}. In practice unigram models are not used, because they are a bad approximation of how texts are generated, and thus yield very bad query likelihood scores.

The other end of the spectrum is to use all the information that one has, to predict the next word. This is of course not possible to story in memory, let alone do computations with it.

\begin{equation}
p(s) = 
\end{equation}

As we can see, words as patterns without any context are too limited as a language model.
In this section we will introduce some language models that use more contextual information than the unigrams models, yet keep the space and time complexity feasible.

\subsection{$n$-grams}
$n$-gram language models generalise unigram language models, by implicitly taking the order of words into account. It models a sentence by taking contigious sequences of $n$ words, where we call $w_1,\ldots,w_{n-1}$ the context $\mathbf{u}$ and $w_n$ the focus word. For the 5-word sentence $s$, \emph{Bananas are my favourite fruit}, we can derive the following $n$-grams:

\begin{enumerate}
	\item Bananas, are, my, favourite, fruit
    \item \emph{Bananas} are, \emph{are} my, \emph{my} favourite, \emph{favourite} fruit
    \item \emph{Bananas are} my, \emph{are my} favourite, \emph{my favourite} fruit
    \item \emph{Bananas are my} favourite, \emph{are my favourite} fruit
    \item \emph{Bananas are my favourite} fruit
\end{enumerate}

The problem here is that if you want to predict in a 5-gram language model that \emph{Bananas} is the first word in a sentence, it must take the role of the focus word. The context is then filled with markers\footnote{These tokens are usually called begin of sentence (BOS) markers, and end of sentence (EOS) markers when they signal the end of a sentence.} that denote they are not part of the sentence. This is useful to query the likelihood of the first $n-1$ words in an $n$-gram language model, but in practice the influence is negligible\footnote{is that so? I believe so.}. In the examples we will mention the markers if they add to the understanding.

The joint probability of $s$ with a bigram language model is thus:
\[ p(s) = p(\mathit{Bananas}|\mathit{BOS})p(\mathit{are}|\mathit{Bananas})p(\mathit{my}|\mathit{are})p(\mathit{favourite}|\mathit{are})p(\mathit{fruit}|\mathit{favourite})\]

Or in a more general and abstract sense:
\[ p(s) = \prod_{i=1}^mp(w_i|w_1,\ldots,w_i-1 \]

Henceforth we will use the abbreviation $w_{i-n+1}^{i-1} \equiv w_{i-n+1},\ldots,w_{i-1}$.

An $n$-gram probability $p(w_i|w_{i-n+1}^{i-1})$ is computed by means of its maximum likelihood estimate (MLE), which is a natural procedure to count how often the token $w_i$ follows the context $w_{i-n+1}^{i-1}$, and to divide by the total number of times the history occurs:
\[ p_{\operatorname{MLE}}\left(w_i|w_{i-n+1}^{i-1}\right) = \frac{c\left(w_{i-n+1}^i\right)}{c\left(w_{i-n+1}^{i-1}\right)} = \frac{c\left(w_{i-n+1}^{i}\right)}{\sum_{w_i}^{V}c\left(w_{i-n+1}^{i}\right)}\]
where $c(\mathbf{u}w)$ is the count function that denotes how often an $n$-gram $\mathbf{u}w$ occurs in the train data.

With the unigram approach, we have to store for each word its probability of occuring. With $W$ the number of words in the vocabulary $V$, we have to store $W$ probabilities. With $n$-grams, there are exponentially many possibilities: $W^n$. A typical vocabulary size is $W\approx 200,000$, hence storing all possibilities for a trigram is already quite a feat. Fortunately, most of these trigrams never occur:\footnote{Although mentioning a $n$-gram that never occurs, is a paradox.} \emph{elephant vaccuum cola}



Hier plaatjes van een corpus met 3,4,5-grammen en de upperbound

These graphs also show sparseness


\subsection{Skipgrams}
A problem of $n$-grams is that it can only model contiguous sequences of words; this rules out any form of long-distance relationship between words. A common example is the interjection of an adjective between a determiner and a noun: \emph{the delicious banana}, \emph{the yellow banana}. To model with, we introduce the skipgram language model, where we now allow $n$-grams to contain a abitrary number of skips, where each skip represents skipping one word. 

and flexgrams

partially solve the sparseness of $n$-grams, by allowing skips.


\section{Smoothing and Backoff Methods}
Earlier we saw that even for a small vocabulary of $50000$ words and a trigram language model, we have to model $O(50000^3)$ parameters. With this number of parameters, we can model the train data very precisely, which as a result causes severe overfitting on train data, especially in the context of maximum-likelihood estimation.
\subsection{Kneser-Ney}


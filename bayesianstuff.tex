\chapter{Introduction to the Bayesian Stuff}

In this chapter we will look at the processes underlying a Bayesian language model. In many other references the work is either considered from a bottom-up approach, or from a top-down approach. The first approach is really useful if you have a strong mathematical background, whereas the latter is much more convenient if you already have (vaguely) heard about the processes involved. The approach of this chapter is to make the processes understandable if neither of the previous two properties are applicable.

Remember that our goal is to devise a language model within a non-parametric Bayesian (BNP) framework. In many statistical inference problems, we expect that structures or patterns continue to emerge, as data accrue, perhaps even without limit. We want a modelling framework that supplies a growing, unbounded number of degrees of freedom to the data analysis. However, if we allow the degrees of freedom to accrue too quickly, we risk finding structures that are statistical artifacts; a typical example of such artifact is overfitting. Overfitting is a serious problem, and it motivates the Bayesian aspect of non-parametrics. In many parametric setting, you choose a number of parameters which optimises a certain value. However, more of different data might already require another number of parameters. The BNP approach is an alternative to this parametric modelling and model selection. Although Bayesian methods are by no means immune to overfitting, they provide a natural resilience to overfitting. By using a model with an unbounded complexity, underfitting is migitated, and computing, or approximating, the full posterior over parameters migitates overfitting.

A common misconception about non-parametrics is that it means that there are no parameters. Instead, it means that the model is not parametric, which in turn means that the number of parameters is not fixed once and for all. If the amount of training data is fixed, there is no difference between parametric and non-parametric models. The difference is there when you add new training data. The non-parametric model can adapt to the influx of new data points, whereas the parametric model does not have the flexibility to add parameters. So the non-parametric framework is not opposed to parameters, to the contrary: the framework can be viewed as allowing an infinite number of parameters, or rather, as an finite but unbounded number of parameters.

The chapter is built as one large explanation of how to build a language model within a Bayesian non-parametric framework. Many of the concepts are introduced sideways, but we tried to separate them into sections as well. If a concept is explained in more detail in another section, we have listed a reference. Although it is a custom in Bayesian literature to first specify a generative model, we start from an example, only to derive at the generative model at the end.

\section{A Bayesian Approach to Language Modelling}

Different from the standard frequentist approach to language modelling, which in essence is based on estimating the maximum likelihood estimate for a pattern, in a Bayesian approach we assume that the pattern comes from a certain distribution and that the texts are generated according to some underlying process. This process is latent, and the same process can generate different texts. The frequentist approach is to model the observed texts, and estimate the probabilities of unseen text based on these observations. The Bayesian way is to infer the underlying process and its pattern distributions. Of course any process could have been used, and if we choose a process, there is no garantuee that it is the best process. One of the most widely used processes to describe such underlying models for textual data is the Chinese Restaurant Process, which we will introduce in the next section. We will show that some of its properties are different from real-life observations, and show how we can adapt the process to model this real-life behaviour.

Just as we have to have a unigram language model to build a $n$-gram language model, for our Bayesian approach, we have to start with a unigram language model as well.

Assume we have a data set $\mathcal{D}$ of $M$ data points $x_1, x_2, \ldots, x_M$, with $W$ unique words comprising a vocabulary $V$, and each word type can be denoted as $v_i$ for $0 < i < W$. The count of word type $v_i$ in $\mathcal{D}$ is denoted as $c(v_i)$. Our data set $\mathcal{D}$ consists of documents which in turn consist of words $x$. We can aggregrate over these words, by storing for each word type $
v_i$ its frequency $c(w_i)$\footnote{$c(v_i) = \sum_{x\in\mathcal{D}} x = v_i$}. In this abstraction we lose the sense of origin, the documents that contained the word, and we lose any positional information. But for our unigram language model, this is of no concern. We will store the words and their frequencies by means of the Chinese restaurant process.

\section{The Chinese Restaurant Process}\marginnote{The Chinese restaurant process describes a family of probability distributions over partitions.}
The Chinese restaurant process (CRP)\footnote{The restaurant metaphore was attributed to Jim Pitman by \cite{Aldous1985Exchangeability}} is one of the many metaphores to make difficult concepts, and their underlying mathematical principles, easier to explain. 

Imagine a restaurant with an infinite number of round tables, with enough space to host an infinite number of guests. Each table in the restaurant serves one dish, but multiple tables may serve the same dish.

Now imagine that each dish $\theta$ represents one of the word types $v_i \in V$, and each customer is a data point $x_j\in\mathcal{D}$ that enters the restaurant, and gets to pick a table $t$. In the case of our unigram language model we have $W$ tables, and customer $x_j$ represents the occurrence of $v_i$, so he sits at the table where $\theta = 
v_i$. Now if all customers $x\in\mathcal{D}$ do this, we end up with at each table as many customers as there are occurrences of that word: $\sum_{\theta=v_i}|t_{\theta}| = c(w_i)$.

Here we can already see the advantage of having a non-parametric framework. If we would have used a parametric model, we could not have dealt with additional data containing new word types: in a parametric model $W$ is fixed. Since we are using a non-parametric model, we now have the means to facilitate additional data without having to resort to another (parametrised) model.

What we have now is a partition of the data points assigned to $K$ clusters.\footnote{Remember that a dish may be served at multiple tables, hence $K\geq W$.} Henceforth we will (interchangeably) call such a partition a seating arrangement $\mathcal{S}$, as the way people sit around the tables forms the partition, with each table being a cluster. More formally, the probability of being assigned to a cluster $k$ is $w_k$, for $k = 1, 2, \ldots, K$ and $\sum_{k=1}^K = 1$, and additionally, each data point can only be in one cluster. The order of clusters in the partition is arbitrary, as is the order of customers within a cluster.\footnote{But we come back to that later when we introduce the notion of exchangeability.} Let $M$ denote the number of customers: $|x\in\mathcal{D}|$, and a partition of size $m$ as $\pi_{[m]}$. 

For example, a restaurant that represents 4 words and that seats 12 customers, might look like this: 
\begin{equation}\label{eq:partitionexample}
	\pi_{[12]} = \{\{1,2,5,9\},\{3,4,8,10\},\{6\},\{7,11,12\}\}.
\end{equation}
This restaurant might for instance represent the data set ``aabbacdbabdd''. 

It is easy to see that for each $m$, we have different seating arrangement $\pi_{[m]}$. Each $m$ describes a different restaurant, so we call the CRP a family of distributions.

The \emph{process} in CRP comes from the generative model. There the CRP is a discrete-time stochastic process whose value at any positive-integer time $m$ is a partition $\pi_{[m]}$ of the set $\{1,2,\ldots,m\}$. This means that the CRP is a sequence of distributions, for $1\leq m \leq\infty$, and if we talk about a specific $m$, the CRP reduces to a Chinese restaurant distribution. The probability distribution is then determined as follows. At time $m=1$ the trivial partition $\pi_{[1]}=\{\{1\}\}$ is obtained with probability $1$. 
Now at time $m+1$ customer $x_{m+1}$ sits at table $t$ with probability $\frac{|t|}{m+1}$, or sits at an empty table probability $\frac{1}{m+1}$: 
\begin{equation}\label{eq:crp-table}
	p(x_{m+1}\text{ seats at }t|\pi_{[m]}) = 
    \begin{cases}
    	\frac{|t|}{m+1}, & \text{if }t\in\pi_{[m]},\\
    	\frac{1}{m+1}, & \text{otherwise}.
  	\end{cases}
\end{equation}

In \cref{eq:crp-table} the probability of joining a table $t$ is expressed. But we can also compute the chance of $x_{m+1}$ seating at $x_i$'s table. Now at time $m+1$ customer $x_{m+1}$ sits at the same table as customer $x_i$ with probability $\frac{1}{m+1}$ for each $i<(n+1)$,\footnote{This can be verified easily: for each of the $|t|$ customers sitting at table $t$, the chance of joining that customer is $\frac{1}{m+1}$, so joining the same table means you have to sum for each customer at table $t$: $\sum_{i=1}^{|t|} \frac{1}{m+1} = \frac{|t|}{m+1}$.} or sits at an empty table with probability $\frac{1}{m+1}$. 

An interesting property of the CRP is exchangeability. Although the CRP is specified using an ordering of the customers, exchangeability states that the distribution on partitions defined by the CRP is invariant to the ordering, in the sense that only the size of the clusters determines the probability of the partition. For example, consider the probability that customers 1 and 2 will be found sitting together at the same table after $M$ customers have entered the restaurant. This probability is $\frac{1}{2}$ because customer 1 sits at an arbitrary table with probability $1$, and customer 2 joins the table with probability $1\cdot\frac{1}{2}$. Now, by exchangeability, this probability does not change if the customers were to enter the restaurant in a different order. Put differently, the probability of any two customers $i$ and $j$ sitting at the same table is $\frac{1}{2}$.

For example for the partition $\pi_{[12]}$ from \cref{eq:partitionexample}, the probability of this assignment is:
\begin{equation}\label{eq:partitionexampleprobability}
	p(\pi_{[12]}) = \frac{1}{1}\frac{1}{2}\frac{1}{3}\frac{1}{4}\frac{2}{5}\frac{1}{6}\frac{1}{7}\frac{2}{8}\frac{3}{9}\frac{3}{10}\frac{1}{11}\frac{2}{12}
\end{equation}
From these products, it also becomes clear that the order in which the customers enter does not matter, as long as they join the same tables. This also follows from the definition of exchangeability, which states that $X_1, X_2, \ldots$ is infinitely exchangeable if for any $n$, $p(X_1,\ldots,X_m)$ is invariant under permutation. More precise: a finite sequence $(X_1, \ldots, X_m)$ of random variables is called exchangeable if $p(X_1,\ldots,X_m) = p(X_{\sigma(1)},\ldots,X_{\sigma(m)})$, for each permutation $\sigma$ of $\{1,\ldots,m\}$. An infinite sequence $(X_1,X_2,\ldots)$ is called exchangeable if $p(X_1,X_2,\ldots)=p(X_{\sigma(1)},X_{\sigma(2)},\ldots)$ for each finite permutation $\sigma$ of $\{1,2,\ldots\}$, that is, each permutation for which $\#\{i: \sigma(i)\neq i\} < \infty$.

The part on infinite exchangeability is relevant because the CRP defines a distribution over ``infinite partitions'', even if you only observe a finite number of data points. The CRP commits to the idea that there is an infinitely large partition. By restraining to a finite number of $x$ data points, we essentially ask how much of the latent infinite partition do we expect to observe if we only have $x$ data points?

We can generalise \cref{eq:partitionexampleprobability} as:
\begin{equation}\label{eq:partitionprobability}
	p(\pi_{[m]}) = \frac{\prod_{t\in\pi{[m]}} (|t|-1)!}{m!}
\end{equation}

So now that we know about exchangeability, we can turn the argument around and recover the CRP update formula in \cref{eq:crp-table} with \cref{eq:partitionprobability}. We add customer $m+1$ to $\pi_{[m]}$ at table $T$, and we call the new partition $\pi'_{[m+1]}$. If $T$ is an existing table, we find that:
\begin{equation}
\begin{split}
	p(x_{m+1}\text{ seats at }T|\pi_{[m]}) &= \frac{p(\pi'_{[m+1]})}{p(\pi_{[m]}} \\
     &= \frac{m!}{(m+1)!}\frac{\prod_{t'\in\pi'_{[m+1]}}(|t'|-1)!}{\prod_{t\in\pi_{[m]}}(|t|-1)!} \\
     &= \frac{1}{m+1}\frac{(|t_0|-1)! \cdot \ldots \cdot (|T|-1)! \cdot\ldots\cdot (|t_K|-1)!}{(|t_0|-1)! \cdot \ldots \cdot (|T-1|-1)! \cdot\ldots\cdot (|t_K|-1)!} \\
     &= \frac{1}{m+1}|T|
\end{split}
\end{equation}

%Here we use the $\infty$-sign colloquially. 
The product $\prod_{t\in\pi_{[m]}}$ is a product over an unbounded number of tables. For the probability of $T$ being empty and $x_{m+1}$ starting a new table, we can do a similar exercise:
\begin{equation}
\begin{split}
	p(x_{m+1}\text{ seats a new table }T|\pi_{[m]}) &= \frac{p(\pi'_{[m+1]})}{p(\pi_{[m]}} \\
    &= \frac{m!}{(m+1)!}\frac{\prod_{t'\in\pi'_{[m+1]}}(|t'|-1)!}{\prod_{t\in\pi_{[m]}}(|t|-1)!} \\
    &= \frac{1}{m+1}\frac{(|t_0|-1)! \cdot\ldots\cdot (|t_K|-1)! \cdot (1-1)!}{(|t_0|-1)! \cdot\ldots\cdot (|t_K|-1)!} \\
	&= \frac{1}{m+1}
\end{split}
\end{equation}

Some readers may notice that although we filled restaurants with customers, and let them take place at tables, we did not mention anything about their food yet. Strictly, serving the dishes is not part of the CRP, but in many literature it is a combined process: serving customers by providing tables and providing dishes. This combined process is also called the CRP mixture model. In this example, and in the remainder of this thesis, we assume that the dishes can be drawn from a distribution.

The idea is that we assign a parameter vector $\phi_t$ to each table $t\in\pi_{[M]}$, and table $t$ hosts $x_i$ if $i\in t$. Additionally, we assume that the data points at table $t$ are generated independently from a common probability distribution indexed by $\phi_t$. So now for each $i\in t$, we let $f(x_i|\phi_t)$ denote the probability density for generating data point $x_i$, and we take the product over $i\in t$ to obtain the total probability of generating the data associated with table $t$. The product over clusters and over data points within clusters is the overall conditional probability of the data:
\begin{equation}
	p(x|\phi,\pi_{[M]}) = \prod_{t\in\pi_{[M]}}\prod_{i\in t} f(x_i|\phi_t),
\end{equation}
with $\phi=(\phi_1,\ldots,\phi_K)$ and $K$ the number of occupied tables. If we fix $x$, then this probability density is known as the likelihood function.

This is almost a complete probability model, however we first need to specify a distribution for the parameters $\phi$. We assume that these parameters are drawn independently from a distribution $G_0$:
\begin{align}
	\pi_{[M]} &\sim \operatorname{CRP}(M)\label{eq:CRPMMpartition} \\
    \phi_t | \pi_{[M]} &\overset{\text{iid}}{\sim} G_0 && \text{ for }t\in\pi_{[M]},\label{eq:CRPMMlatent} \\
    x_i|\phi,\pi_{[M]} &\overset{\text{ind}}{\sim} F(\phi_t) && \text{ for }t\in\pi_{[M]}\text{ and }i\in t,\label{eq:CRPMMdatapoints}
\end{align}
with $F(\phi_t)$ being the distribution with density $f(\cdot|\phi_t)$. The linked conditional probabilities yield a joint probability distribution on the collection of variables $(x,\phi,\pi_{[M]})$. Naturally, we are mostly interested in the posterior probability of $\pi_{[M]}$ given $x$.

This model is called the CRP mixture model because in a mixture model each data point is generated from one of a set of mixture components, and the choice of mixture component is made randomly for each data point. In this case, $\phi$ defines the mixture components, and $\pi_{[M]}$ selects randomly among these mixture components by assigning a table to each data point, and each data point is generated from the selected mixture component with a draw from $F(\phi_t)$.

The expected number of tables as $M \rightarrow \infty$ is... This can be seen as follows: let $\tau_i$ be the event that the $i$th customer starts a new table. The probability of this happening is $p(\tau_i = 1) = \frac{1}{(i-1)+1}$. We can denote the number of tables $K$ after $n$ customers as $k_m = \sum_i \tau_i$, which is equal to $\sum_i \frac{1}{(i-1)+1}$ which is in turn upper bounded by $O(H_m)$ where $H_m$ is the $m$th harmonic sum\footnote{The harmonic series is defined as: $\sum_{m=1}^\infty \frac{1}{m} = 1 + \frac{1}{2} + \frac{1}{3} + \cdots$. The $m$th partial sum is then $H_m=\sum_{k=1}^m \frac{1}{k}$.} which grows logarithmically with $O(\ln m+1)$. 



\section{Stick-breaking}\marginnote{The stick-breaking process describes a probability distribution over partition sizes.}
In the previous section we defined a process to generate a distribution over partitions by means of a seating arrangement. In this section we introduce a new metaphore called the stick-breaking distribution which defines the distribution of customers sitting at the tables, without generating an explicit seating arrangement.

The stick-breaking process\cite{Ishwaran2001Gibbs} is a distribution on the infinite sequence  $\xi = (\xi_1,\xi_2,\ldots)$; it is also known as the GEM distribution.\footnote{The GEM distribution is named after Griffiths, Engen, and McCloskey, cf.\ \cite{Pitman1997The}}

The metaphore is as follows: imagine you have a stick of length $1$. You generate $\xi_1$ by snapping the stick into two pieces. The length of one of the two pieces becomes the value $\xi_1$. To generate $\xi_2$ you then snap the other piece in two; again the length of one of these pieces becomes $\xi_2$. As you repeat this stick-breaking, the length of the stick that you have left approaches zero.

$\xi_k$ can then be interpretated as the length of the piece of unit-length stick assigned to the $k$th value.\footnote{$k$ can be considered to be the $k$th table in the CRP, with $\xi_k$ being the number of customers seated at $t_k$.} After the first $(k-1)$ values have their portions assigned, the length of the remainder of the stick, $\prod_{i=1}^{k-1}(1-\xi'_i)$, is broken according to a sample $\xi'_k$ from a beta distribution, where $\xi'_k$ indicates the portion of the remainder to be assigned to the $k$th value. The beta distribution is defined as follows:
\begin{equation} 
	\operatorname{Beta}(x; \alpha, \beta) = \frac{1}{\operatorname{B}(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}\label{eq:betad}
\end{equation}
with $\alpha,\beta >0$ and $\operatorname{B}(\alpha,\beta)$ being the beta function
\begin{equation}
	\operatorname{B}(\alpha,\beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\label{eq:betaf}
\end{equation}
and for $\Gamma(n)$ denoting the Gamma function for $n>0$:
\begin{equation}
	\Gamma(n) = (n-1)!\label{eq:gammaf}
\end{equation}
Just as in the CRP, the probability space is uniformly distributed over the number of customers. This uniform distribution can be represented as a beta distribution with parameters $(1,1)$, such that 
\begin{equation}
	\operatorname{Beta}(1,1)=\frac{1}{\operatorname{B}(1,1)} = \frac{1}{(1-1)!} = 1
\end{equation}

Now let $(\phi_1, \phi_2, \ldots)$ be a set of samples from $G_0$.\footnote{Compare this to \cref{eq:CRPMMlatent}, but now $t\rightarrow\infty$, so we do not have to use $\pi_{[M]}$ anymore, and its conditional independence can be dropped.} The distribution given by density 
\begin{equation}
	f(\phi) = \sum_{k=1}^\infty \xi_k \cdot \delta_{\phi}(\phi) 
\end{equation}
with $\delta(\cdot)$ being the Dirac delta measure, used as an indicator function. It is clear from the construction of $f$ that a non-parametric sample is generated, and that the samples are discrete (from the indicator function).



\section{Urn Representation}
Another way to fill the Chinese restaurant is by means of drawing balls from an urn. Different from the CRP where we assigned each customer to its table, with the urn representation we have a process where we select a table, and consequently assign a customer to that table.

The urn works as follows. An urn contains objects of various types, where the objects are customarily called the balls, and each ball type can be distinguished by its colour. In general the content of an urn can change over time, subject to rules of placing balls into the urn, or drawing balls under predesignated replacement schemes. A simple example is when there is a particular distribution $P$ of balls in the urn, and you immediately replace the drawn ball after noting its colour; drawing a ball from the urn is then the same as drawing from distribution $P$. In this case $P$ is a fixed distribution. By adding or removing balls after each draw, the distribution can also be dynamic.

In the section on CRP mixture models we saw that the underlying distributions were far from static, hence we need a replacement scheme that matches this distribution: Hoppe-P\'olya urn.\footnote{The Hoppe-P\'olya urn is a distribution on sequences of random variables.} In the literature this scheme is often simply called the P\'olya urn.\footnote{Compare this to the interchangeability of CRP and the Dirichlet Process CRP in the literature. We will also undo this unclarity in a later section.} In this section we will introduce the P\'olya urn, the Hoppe-P\'olya urn, and the missing link between the two urn schemes, the multivariate P\'olya urn scheme.

The P\'olya urn scheme\cite{Johnson1977Urn} is the simplest of the three schemes. The urn contains only white and black balls. For each ball that is drawn (randomly) from the urn, we observe its colour,  replace\footnote{Read: put back into the urn.} it, and add an additional ball of the same colour to the urn. 

A modest generalisation of the P\'olya urn scheme is the multivariate P\'olya urn scheme. Here the generalisation entails that the balls can have other colours as well, although each ball still has only one colour. Image a Chinese restaurant where the number of tables is limited to $\kappa$ and $\kappa$ unique dishes are served; each table has a unique dish. The restaurant hosts $\nu$ customers. We can translate this scenario to the multivariate P\'olya urn, where we have $\kappa$ different colours, and $\nu$ balls. In the CRP\footnote{And in our reduced example.} we modelled the types as tables, and the tokens as the customers sitting around the same table. In the urn metaphore, the colours are the types, and the number of balls with a colour as the tokens. Since we limited $K=\kappa$ in this example, customers cannot choose to join a new table. The chance of joining another word is now $\frac{1}{\nu}$, and the chance of being of the same colour $h$ as ball $b$ is $\frac{c(h)}{\nu}$ with $c(x)$ denoting the count of balls with colour $x$. 

More interesting is a generalisation of the P\'olya urn scheme that can behave the same way as a CRP. With the multivariate P\'olya urn scheme, we came close, but lacked the means to generate new tables.\footnote{And thus also the means to generate new dishes.} With the Hoppe-P\'olya urn scheme\cite{Hoppe1984Polya}, we can overcome this problem as follows. We start with an urn with one black\footnote{Read black as uncoloured, as we also do not take it into account when we count the number of different colours in the urn.} ball. When drawing from the urn, if we draw a black ball, we replace the ball, and alongside add a new ball of a new non-black colour randomly generated from a probability distribution. Otherwise, we put the ball back along with another ball of the same colour, as for the standard multivariate P\'olya urn scheme. The colours of an infinite sequence of draws from the Hoppe-P\'olya urn scheme follow a CRP. If instead of generating new colours (tables in the CRP), we draw a random value from a given base distribution such as $G_0$, we can model a CRPMM.\footnote{Compare this to \cref{eq:CRPMMlatent}. Also note that the P\'olya urn scheme models a beta-binomial distribution, with the parameters of the beta distribution depending on the initial layout of the urn. The multivariate P\'olya urn models a Dirichlet-multinomial distribution, where we set the Dirichlet parameter vector $\alpha=1$. The Hoppe-P\'olya urn scheme models the Dirichlet process, with $\alpha=1$. We will come back to this in a later section on the Dirichlet Process.} Similar to the CRP, the number of colours grows logarithmically to infinity, as the expected number of colours at time $n$ is $\sum_{i=1}^n i^{-1}$

\textcite{Blackwell1973Ferguson} define the P\'olya urn scheme as follows. Let $\mu$ be any finite positive measure on a complete separable metric space $X$, and $(B_1,\dots,B_r)$ a finite partition of $X$. Now every sequence $\{X_n,n\geq 1\}$ of random variables with values in $X$ is a P\'olya sequence with parameter $\mu$ if for every $B\subset X$:
\begin{equation}
	p(X_1\in B) = \frac{\mu(B)}{\mu(X)}
\end{equation}
and
\begin{equation}
	p\{X_{n+1}\in B | X_1,\ldots,X_n\} = \frac{\mu_n(B)}{\mu_n(X)}.
\end{equation}
with $\mu_n = \mu + \sum_{i=1}^n \delta(X_i)$ and $\delta(x)$ denoting the unit measure concentrating at $x$. For every finite $X$, the sequence $\{X_n\}$ represents the results of successive draws from an urn where initially the urn has $\mu(x)$ balls of colour $x$, and after each draw, the ball drawn is replaced and another ball of its same colour is added to the urn.

The Hoppe-P\'olya stochastic process can be formalised as follows. Let $C = \{1,2,\ldots\}$ be the set of colours and $X_n$ the random variable which indicates the colour of the new ball added to the urn after the $n$th draw. Let $K$ be the random number of distinct colours that are in the urn at time $n$, then the urn can be described as a configuration of $n$ tokens in $K$ cells $\{n_1, n_2, \ldots, n_K\}$ where $\sum_{i=1}^K n_i = n$ are the occupancy numbers of the $K$ cells, which form a partition\footnote{A partition of an integer $n$, is one way of writing $n$ as the sum of positive integers where the
order of the addends (terms being added) does not matter.} of the integer $n$. The sequence $\{X_j\}_{j=1}^n$ determines the random partition $\pi_{n}$.

Consider a partition $a = (a_1,a_2,\ldots,a_n)$ with $K$ distinct colours and a sequence of draws $\pi_{n} = \{X_j\}_{j=1}^n$ generating it. The probability of obtaining this sequence is then:
\begin{equation}
	p(\pi_n) = \frac{1^K}{n!}\prod_{j=1}^K (n_j-1)!
\end{equation}
The $\frac{1}{n!}$ comes from the fact that in order to get $K$ colours, the black ball must be drawn $K$ times. Then in order to get exactly the required partition, each colour must be drawn at exactly $n_j-1$ times. Not surprisingly, but a confirmation nonetheless, this models the same probability as \cref{eq:partitionprobability}: the probability of a partition in the CRP.

%http://ofce-skema.org/wp-content/uploads/2013/06/marengo.pdf
%https://stat.duke.edu/courses/Fall09/sta205/lec/exchange.pdf

%http://www.stat.berkeley.edu/~pitman/blmq.pdf
%
%http://www.artofproblemsolving.com/Resources/Papers/LaurendiPartitions.pdf
%http://mlss11.bordeaux.inria.fr/docs/mlss11-Teh.pdf


\section{The Dirichlet Process: a Complete Prior}
In the Chinese restaurant process we observed a language model from the perspective of the customers, representing the word occurrences. There we saw that according to the CRP, they all have an equal probability of joining another customer, and the probability of joining an existing table was proportional to the number of customers already sitting at that table. In the Hoppe-P\'olya urn scheme, we watched the same process from the table point-of-view; here we saw customers joining tables or starting new tables. By means of the stick-breaking process, we observed a way to describe the distribution over number of customers sitting at tables.

Now, saying that customers have an equal likelihood of sitting at eachother's tables is a strong limitation. Therefore we introduce a parametrised generalisation of the previous priors, called the Dirichlet Process, that encompasses all of the above. Historically the Dirichlet process was invented first, and the other processes were all invented to provide tractable methods for using the Dirichlet process. 

To come to the Dirichlet process from the line of processes that we followed in this thesis, we introduce a concentration parameter $\alpha $. If we set $\alpha = 1$, we reduce the Dirichlet process to a process coherent with the CRP, stick-breaking process, and Hoppe-P\'olya urn scheme that we saw in the previous sections.  

The Dirichlet Process is a way of assigning a probability distribution over probability distributions. It is similar to the CRP, the stick-breaking process, and the urn schemes, in a sense that it is often used as a prior distribution. In this section we will briefly introduce the Dirichlet Process, and show the relations and differences with the priors we introduced in the previous sections.

We will start with a somewhat informal introduction. The Dirichlet Process is a stochastic process, and is often coined the cornerstone of Bayesian nonparametric models. It represents a distribution over distributions, such that each draw from a Dirichlet process is a distribution itself. It is called a Dirichlet process because it has Dirichlet distributed finite dimensional marginal distributions. The distributions drawn from a Dirichlet process are discrete, but cannot be described using a finite number of parameters, hence the Dirichlet process is classified as a nonparametric model.

\subsection{Dirichlet Process}

Assume we have a corpus $\mathcal{C}$ with $W$ words and vocabulary $V$, and we want to compare two documents $\mathcal{D}_1$ and $\mathcal{D}_2$ from this corpus. In a Bayesian setting we again assume some underlying probability mass function $q$ generating the documents, sampled from the random probability mass function $Q$, such that $q_1,q_2\in Q$. With the Dirichlet distribution we can model the randomness between the two documents. If we assume that all words have an equal probability, then we can imagine a fair $W$-sided die. A document is then generated by rolling the die, and each outcome is a word token $v\in V$. The die can be represented by a probability mass function of length $W$, with a uniform distribution. Now we have for both $\mathcal{C}_1$ and $\mathcal{C}_2$ a probability mass function, and we can use the Dirichlet distribution to capture the variability.

In practice the distribution of words is not uniform, and the probability mass function is then of length $W$ but normalised over the empirical word frequencies. 

More formally, let $Q = [Q_1, Q_2, \ldots, Q_K]$ be a random pmf with $K$ components, with $Q_i \geq 0$ for $i = 1,2,\ldots,K$ and $\sum_{i=1}^K Q_i = 1$. In addition, suppose that $\alpha = [\alpha_1,\alpha_2,\ldots,\alpha_K]$,\footnote{Although perhaps confusing, this is not necessarily the same $\alpha$ as the Dirichlet process concentration parameter, although in practice this is often the case. Here we just follow the convention of the literature, where $\alpha$ is also used to denote the concentration parameter of the Dirichlet distribution.} with $\alpha_i > 0$ for each $i$ and let $\alpha_0 = \sum_{i=1}^K \alpha_i$.\footnote{If $\alpha_1=\alpha_2=\ldots=\alpha_K$, then we call $\alpha$ symmetric, and the distribution a symmetric Dirichlet distribution.} Then $Q$ is said to have a Dirichlet distribution with parameter $\alpha$, which we can denote by $Q \sim \operatorname{Dir}(\alpha)$:\footnote{See Section 2 of \citet{Ferguson1973A} for a more elaborate introduction, with many technicalities that we assume to be true; where not, we will indicate this.\cite{Ferguson1973A}}
\begin{equation}
	\operatorname{Dir}(q; \alpha) = \frac{\Gamma(\alpha_0)}{\prod_{i=1}^K \Gamma(\alpha_i)} \prod_{j=1}^K q_i^{\alpha_j-1}
\end{equation}
with $\Gamma(\cdot)$ denoting the gamma function (see~\cref{eq:gammaf}).

The Dirichlet distribution can be thought of as a probability distribution over the $(K-1)$-dimensional probability simplex\footnote{The $k$-simplex is a geometrical $(k-1)$-dimensional object ($k$ follows from $k=1-\sum_{i=1}^{k-1} q_i$), which is a generalisation of the notion of a triangle to $k$ dimensions. It is defined as the point set consisting of the convex hull of a set of linear independent points in $\mathbb{R}^k$, with its $k$ components being non-negative and summing to $1$: $\Delta_k = \{q\in\mathbb{R}^k|\sum_{i=1}^k q_i=1,q_i\geq0\text{ for }i=1,2,\ldots,k\}$.} $\Delta_K$; put otherwise, as a distribution over pmfs of length $K$. With $K=2$, the Dirichlet distribution reduces to a Beta distribution (see~\cref{eq:betad}), thus if $X\sim\operatorname{Beta}(a,b)$ then $Q=(X,1-X)\sim\operatorname{Dir}(\{a,b\})$, and vice versa.

With $\alpha = \{1\}_{i=1}^K$, the Dirichlet distribution reduces to the uniform distribution over the simplex. When the components of $\alpha$ are all greater then $1$, the density has one mode being somewhere in the interior of the simplex. With the components of $\alpha$ being all less than $1$, the density has sharp peaks around the edges of the simplex. However it should be noted that the support of the Dirichlet is open, and does not include the edges of the simplex, and therefore a component drawn from a Dirichlet will never be $0$. Note the difference between $\alpha$ being only $1$s, and all $\alpha_i$ going to infinity. In the first case all sets of distributions are equally likely\footnote{The distribution over distributions is uniform.}, whereas in the latter case only near-uniform distributions\footnote{The distribution over distributions is peaked around the uniform distribution.} are likely.

One of the properties of the Dirichlet distribution is that it is the conjugate prior for the multinomial distribution. The multinomial distribution is parametrised by an integer $n$ and a pmf $q = [q_1, q_2, \ldots, q_3]$. $n$ denotes the number of independent events, and for each event, the probability of outcome $i$ is $q_i$. The multinomial distribution then specifies the probability that outcome $i$ occurs $x_i$ times, for $i=1,2,\ldots,K$.\footnote{The multinomial distribution can model the probability of an $n$-sample empirical histogram, if each sample is drawn iid from $q$.} If $X\sim\operatorname{Mult}(n,q)$, then its probability mass function is given by
\begin{equation}
	\operatorname{Mult}(x_1,x_2,\ldots,x_K|n, q_1,q_2,\ldots,q_K) = \frac{n!}{x_1!x_2!\ldots x_K!}\prod_{i=1}^K q_i^{x_i}
\end{equation}
when $\sum_{i=1}^K x_i = n$, $0$ otherwise. We can also express this with gamma functions to shows it correspondence to the Dirichlet distribution:
\begin{equation}
	\operatorname{Mult}(x_1, x_2,\ldots,x_K|n, q_1,q_2,\ldots,q_K) = \frac{\Gamma(\sum_{i=1}^K x_i+1)}{\prod_{i=1}^K \Gamma(x_i+1)}\prod_{i=1}^K q_i^{x_1}.
\end{equation}

\begin{equation}
	\left(\frac{\Gamma(\sum_{i=1}^K x_i+1)}{\prod_{i=1}^K \Gamma(x_i+1)}\prod_{i=1}^K q_i^{x_1}\right)\left(\frac{\Gamma(\alpha_0)}{\prod_{i=1}^K \Gamma(\alpha_i)} \prod_{j=1}^K q_i^{\alpha_j-1}\right)
\end{equation}

Now consider an example in which we have $L$ documents, each assigned to a pmf $q^{(1)}, \ldots, q^{(L)}$, each $q^{(i)} \overset{\text{iid}}{\leftarrow} Q$, and $Q\sim\operatorname{Dir}(\alpha)$. If we draw iid $n_i$ samples from each pmf $q^{(i)}$
\begin{equation}
	\operatorname{Dir}(\alpha) \overset{\text{iid}}{\rightarrow}
    \begin{cases}
    q^{(1)} \overset{\text{iid}}{\rightarrow} x_{1,1}, x_{1,2},\ldots,x_{1,n_1} \triangleq x_1\\
    q^{(2)} \overset{\text{iid}}{\rightarrow} x_{2,1}, x_{2,2},\ldots,x_{2,n_2} \triangleq x_2 \\
    \vdots \\
    q^{(L)} \overset{\text{iid}}{\rightarrow} x_{L,1}, x_{L,2},\ldots,x_{L,n_L} \triangleq x_L \\
  \end{cases}
\end{equation}
then $\{x_i\}$ are the realisation of a compound Dirichlet distribution, also known as the multivariate P\'olya distribution that we saw earlier with $L$ different balls and $n_i$ colours. Now the distributions over distributions is not uniform, but concentrated around certain distributions by means of the concentration parameter $\alpha$.

\subsection{The Dirichlet distribution as a parametrised prior}

In the previous sections we discussed the CRP, stick-breaking process, and urn schemes. In the remainer of this section we briefly discuss the same concepts, but now in the context of the Dirichlet distribution with the new concentration parameter $\alpha$. 

If we start with the urn scheme, then we want to generate a realisation $Q\sim\operatorname{Dir}(\alpha)$. That is, we put $\alpha_i$ balls of colour $i$ for $i=1,2,\ldots,K$ in an urn. Note that $\alpha_i>0$ and is not necessarily an integer. The process of drawing balls is similar to the multivariate P\'olya urn discussed earlier, but now we can have a different number of coloured balls.\footnote{The distribution is not uniform anymore, but now models a Dirichlet-multinomial distribution with concentration parameter $\alpha$.} At each iteration, we draw one ball uniformly at random from the urn, and then place it back into the urn along with an additional ball of the same colour. As we iterate this process, the proportions of balls of each colour will converge to a pmf that is a sample from the distribution $\operatorname{Dir}(\alpha)$. Let $Q_n = (Q_{n1}, Q_{n2},\ldots,Q_{nk})$, where $Q_{ni}$ is the proportion of balls of colour $i$ after $n$ balls are in the urn. Then $Q_n \rightarrow_\text{d} Q \sim \operatorname{Dir}(\alpha)$ as $n\rightarrow\infty$, with $\rightarrow_\text{d}$ denoting convergence in distribution.\footnote{$p(Q_{n1}\leq z_1, Q_{n2}\leq z_2, \ldots, Q_{nk}\leq z_k) \rightarrow p(Q_1\leq z_1, Q_2\leq z_2,\ldots,Q_K\leq z_K)$ as $n\rightarrow\infty$ for all $(z_1,z_2,\ldots,z_3)$.} 

%For the parametric version of the CRP, with the number of tables and dishes fixed, the process of introducing the concentration parameter $\alpha$ is quite similar. In a restaurant with $n$ customers, the probability of joining a table $t$ is still proportional to $|t|$. However, the chance of joining a new table is now proportional to $\alpha$. The new probabilities are thus:
%\begin{itemize}
%	\item Joining another customer: $\frac{1}{n+\alpha}$;
%    \item Joining table $t$: $\frac{|t|}{n+\alpha}$;
%    \item Starting a new table: $\frac{\alpha}{n+\alpha}$.
%\end{itemize}

For the CRP in a parametric setting, with the number of tables and dishes fixed, there is no analog for introducing the concentration parameter.




\section{Pitman-Yor Process}
